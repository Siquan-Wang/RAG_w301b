"""
å®Œæ•´çš„ PDF RAG ç³»ç»Ÿ
æ”¯æŒæ–‡æœ¬ã€å›¾åƒå’Œè¡¨æ ¼å¤„ç†
"""
import os
import fitz  # PyMuPDF
import pdfplumber
from openai import OpenAI
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import base64
from datetime import datetime

load_dotenv()

# é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=OPENAI_API_KEY)
es = Elasticsearch(['http://localhost:9200'], verify_certs=False)
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

class PDFProcessor:
    """å¤„ç† PDF æ–‡æ¡£"""
    
    def __init__(self, index_name):
        self.index_name = index_name
        self.setup_index()
    
    def setup_index(self):
        """åˆ›å»ºç´¢å¼•"""
        try:
            if es.indices.exists(index=self.index_name):
                es.indices.delete(index=self.index_name)
        except:
            pass
        
        mapping = {
            "mappings": {
                "properties": {
                    "text": {"type": "text"},
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 384,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "source": {"type": "keyword"},
                    "page": {"type": "integer"},
                    "content_type": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"}
                }
            }
        }
        es.indices.create(index=self.index_name, **mapping)
        print(f"âœ“ ç´¢å¼•åˆ›å»ºæˆåŠŸ: {self.index_name}")
    
    def extract_text(self, pdf_path):
        """æå–æ–‡æœ¬"""
        print("\n[1/3] æå–æ–‡æœ¬...")
        chunks = []
        
        doc = fitz.open(pdf_path)
        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text()
            
            if text.strip():
                # ç®€å•åˆ†å—ï¼ˆæ¯500å­—ç¬¦ä¸€å—ï¼‰
                words = text.split()
                chunk_size = 100  # å•è¯æ•°
                
                for i in range(0, len(words), chunk_size):
                    chunk_text = ' '.join(words[i:i+chunk_size])
                    if chunk_text.strip():
                        chunks.append({
                            'text': chunk_text,
                            'source': os.path.basename(pdf_path),
                            'page': page_num + 1,
                            'content_type': 'text'
                        })
        
        doc.close()
        print(f"âœ“ æå–äº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def extract_images(self, pdf_path):
        """æå–å›¾åƒå¹¶ç”Ÿæˆæè¿°"""
        print("\n[2/3] æå–å›¾åƒ...")
        image_data = []
        
        doc = fitz.open(pdf_path)
        image_count = 0
        
        for page_num in range(min(len(doc), 5)):  # åªå¤„ç†å‰5é¡µä»¥èŠ‚çœæˆæœ¬
            page = doc[page_num]
            image_list = page.get_images()
            
            for img_index, img in enumerate(image_list[:2]):  # æ¯é¡µæœ€å¤š2å¼ å›¾
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ç”Ÿæˆæè¿°
                    caption = self.caption_image(image_bytes, page_num + 1)
                    
                    image_data.append({
                        'text': f"å›¾åƒæè¿°: {caption}",
                        'source': os.path.basename(pdf_path),
                        'page': page_num + 1,
                        'content_type': 'image'
                    })
                    image_count += 1
                except Exception as e:
                    print(f"  è·³è¿‡å›¾åƒ {img_index}: {e}")
        
        doc.close()
        print(f"âœ“ å¤„ç†äº† {image_count} å¼ å›¾åƒ")
        return image_data
    
    def caption_image(self, image_bytes, page_num):
        """ä½¿ç”¨ GPT-4 Vision ç”Ÿæˆå›¾åƒæè¿°"""
        try:
            base64_image = base64.b64encode(image_bytes).decode('utf-8')
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "ç®€è¦æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ˆ1-2å¥è¯ï¼‰ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }],
                max_tokens=100
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"ç¬¬ {page_num} é¡µçš„å›¾åƒ"
    
    def extract_tables(self, pdf_path):
        """æå–è¡¨æ ¼"""
        print("\n[3/3] æå–è¡¨æ ¼...")
        table_data = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages[:5]):  # åªå¤„ç†å‰5é¡µ
                    tables = page.extract_tables()
                    
                    for table in tables:
                        if table and len(table) > 1:
                            # è½¬æ¢ä¸ºæ–‡æœ¬
                            table_text = self.table_to_text(table)
                            
                            table_data.append({
                                'text': f"è¡¨æ ¼å†…å®¹: {table_text}",
                                'source': os.path.basename(pdf_path),
                                'page': page_num + 1,
                                'content_type': 'table'
                            })
        except Exception as e:
            print(f"  è¡¨æ ¼æå–å¤±è´¥: {e}")
        
        print(f"âœ“ æå–äº† {len(table_data)} ä¸ªè¡¨æ ¼")
        return table_data
    
    def table_to_text(self, table):
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬"""
        lines = []
        for row in table[:5]:  # åªå–å‰5è¡Œ
            row_text = ' | '.join([str(cell or '') for cell in row])
            lines.append(row_text)
        return '\n'.join(lines)
    
    def index_documents(self, documents):
        """ç´¢å¼•æ–‡æ¡£"""
        print(f"\nç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")
        
        for i, doc in enumerate(documents):
            try:
                embedding = model.encode(doc['text']).tolist()
                
                doc_body = {
                    'text': doc['text'],
                    'embedding': embedding,
                    'source': doc['source'],
                    'page': doc['page'],
                    'content_type': doc['content_type'],
                    'chunk_id': f"{doc['source']}_p{doc['page']}_{i}"
                }
                
                es.index(index=self.index_name, id=f"doc_{i}", document=doc_body)
                
                if (i + 1) % 10 == 0:
                    print(f"  å·²ç´¢å¼• {i + 1}/{len(documents)}")
            except Exception as e:
                print(f"  ç´¢å¼•å¤±è´¥ {i}: {e}")
        
        es.indices.refresh(index=self.index_name)
        print(f"âœ“ ç´¢å¼•å®Œæˆ")
    
    def process_pdf(self, pdf_path):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        print(f"\nå¤„ç† PDF: {pdf_path}")
        print("="*70)
        
        # æå–æ‰€æœ‰å†…å®¹
        text_chunks = self.extract_text(pdf_path)
        image_data = self.extract_images(pdf_path)
        table_data = self.extract_tables(pdf_path)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
        all_docs = text_chunks + image_data + table_data
        
        # ç´¢å¼•
        self.index_documents(all_docs)
        
        print(f"\nå¤„ç†å®Œæˆ!")
        print(f"  æ–‡æœ¬å—: {len(text_chunks)}")
        print(f"  å›¾åƒ: {len(image_data)}")
        print(f"  è¡¨æ ¼: {len(table_data)}")
        print(f"  æ€»è®¡: {len(all_docs)} ä¸ªæ–‡æ¡£")
        
        return len(all_docs)


class RAGQuery:
    """RAG æŸ¥è¯¢"""
    
    def __init__(self, index_name):
        self.index_name = index_name
    
    def search(self, query, top_k=5):
        """æœç´¢"""
        query_embedding = model.encode(query).tolist()
        
        result = es.search(
            index=self.index_name,
            body={
                "knn": {
                    "field": "embedding",
                    "query_vector": query_embedding,
                    "k": top_k,
                    "num_candidates": 50
                },
                "_source": ["text", "source", "page", "content_type"]
            }
        )
        
        docs = []
        for hit in result['hits']['hits']:
            docs.append({
                'text': hit['_source']['text'],
                'source': hit['_source']['source'],
                'page': hit['_source']['page'],
                'type': hit['_source']['content_type'],
                'score': hit['_score']
            })
        
        return docs
    
    def generate_answer(self, query):
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ£€ç´¢
        docs = self.search(query)
        
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯", []
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[æ–‡æ¡£{i+1}] (æ¥æº: {doc['source']}, ç¬¬{doc['page']}é¡µ, ç±»å‹: {doc['type']})\n{doc['text'][:300]}"
            for i, doc in enumerate(docs)
        ])
        
        # ç”Ÿæˆç­”æ¡ˆ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªhelpfulçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶æ ‡æ³¨å¼•ç”¨æ¥æºã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"
                },
                {
                    "role": "user",
                    "content": f"é—®é¢˜: {query}\n\nå‚è€ƒæ–‡æ¡£:\n{context}\n\nè¯·å›ç­”:"
                }
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        answer = response.choices[0].message.content
        return answer, docs


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ å®Œæ•´ PDF RAG ç³»ç»Ÿ")
    print("="*70)
    
    # æ£€æŸ¥ PDF æ–‡ä»¶
    pdf_dir = "test_pdf"
    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
    
    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]
    
    if not pdf_files:
        print(f"\nâš ï¸  è¯·å°† PDF æ–‡ä»¶æ”¾åˆ° '{pdf_dir}' ç›®å½•ä¸­")
        input("\næŒ‰å›è½¦é€€å‡º...")
        return
    
    print(f"\næ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶:")
    for i, f in enumerate(pdf_files, 1):
        print(f"  {i}. {f}")
    
    # é€‰æ‹©æ–‡ä»¶
    if len(pdf_files) == 1:
        pdf_path = os.path.join(pdf_dir, pdf_files[0])
    else:
        choice = input(f"\né€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶ (1-{len(pdf_files)}): ").strip()
        try:
            idx = int(choice) - 1
            pdf_path = os.path.join(pdf_dir, pdf_files[idx])
        except:
            print("æ— æ•ˆé€‰æ‹©")
            return
    
    index_name = "pdf_rag_index"
    
    # å¤„ç† PDF
    processor = PDFProcessor(index_name)
    processor.process_pdf(pdf_path)
    
    # äº¤äº’å¼é—®ç­”
    print("\n" + "="*70)
    print("ğŸ“š PDF å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥æé—®äº†")
    print("="*70)
    print("è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰\n")
    
    rag = RAGQuery(index_name)
    
    while True:
        try:
            question = input("ğŸ’¬ ä½ çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit', 'q']:
                break
            
            print("\nğŸ” æ­£åœ¨æ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆ...\n")
            
            answer, docs = rag.generate_answer(question)
            
            print("="*70)
            print("ğŸ“ AI å›ç­”:")
            print("="*70)
            print(answer)
            
            print("\n" + "="*70)
            print(f"ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£ (å…±{len(docs)}ä¸ª):")
            print("="*70)
            for i, doc in enumerate(docs, 1):
                print(f"\n[æ–‡æ¡£{i}] {doc['source']} - ç¬¬{doc['page']}é¡µ - {doc['type']}")
                print(f"ç›¸ä¼¼åº¦: {doc['score']:.4f}")
                print(doc['text'][:200] + "...")
            
            print("\n" + "="*70 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")
    
    # æ¸…ç†
    cleanup = input("\næ˜¯å¦åˆ é™¤ç´¢å¼•ï¼Ÿ(y/n): ").strip().lower()
    if cleanup == 'y':
        es.indices.delete(index=index_name)
        print(f"âœ“ å·²åˆ é™¤ç´¢å¼•: {index_name}")
    
    print("\nå®Œæˆï¼")


if __name__ == "__main__":
    main()

