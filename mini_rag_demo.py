"""
æœ€å°åŒ– RAG æ¼”ç¤º
ä¸éœ€è¦ PDFï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬è¿›è¡Œæµ‹è¯•
"""
from openai import OpenAI
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv

load_dotenv()

print("="*70)
print("ğŸš€ æœ€å°åŒ– RAG ç³»ç»Ÿæ¼”ç¤º")
print("="*70)

# é…ç½®
INDEX_NAME = "mini_rag_demo"
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# åˆå§‹åŒ–
print("\n[1/6] åˆå§‹åŒ–ç»„ä»¶...")
es = Elasticsearch(['http://localhost:9200'], verify_certs=False)
client = OpenAI(api_key=OPENAI_API_KEY)

# ä½¿ç”¨è½»é‡çº§åµŒå…¥æ¨¡å‹
print("åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼Œçº¦ 120MBï¼‰...")
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
print("âœ“ ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

# åˆ›å»ºç´¢å¼•
print("\n[2/6] åˆ›å»ºç´¢å¼•...")
try:
    if es.indices.exists(index=INDEX_NAME):
        es.indices.delete(index=INDEX_NAME)
except:
    pass

mapping = {
    "mappings": {
        "properties": {
            "text": {"type": "text"},
            "embedding": {
                "type": "dense_vector",
                "dims": 384,
                "index": True,
                "similarity": "cosine"
            }
        }
    }
}
es.indices.create(index=INDEX_NAME, **mapping)
print(f"âœ“ ç´¢å¼•åˆ›å»ºæˆåŠŸ: {INDEX_NAME}")

# å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
print("\n[3/6] å‡†å¤‡çŸ¥è¯†åº“...")
documents = [
    "RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ã€‚å®ƒå…ˆä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶ååŸºäºè¿™äº›æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆã€‚",
    "Elasticsearch æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æœç´¢å¼•æ“ï¼Œæ”¯æŒå…¨æ–‡æœç´¢å’Œå‘é‡æœç´¢ã€‚åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œå®ƒç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£ã€‚",
    "å‘é‡åµŒå…¥ï¼ˆVector Embeddingï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—å‘é‡ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£æ–‡æœ¬çš„è¯­ä¹‰å«ä¹‰ã€‚ç›¸ä¼¼çš„æ–‡æœ¬ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡ã€‚",
    "GPT æ˜¯ OpenAI å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ã€‚åœ¨ RAG ä¸­ï¼ŒGPT è´Ÿè´£æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚",
    "æ··åˆæœç´¢ç»“åˆäº†å…³é”®è¯æœç´¢å’Œå‘é‡æœç´¢çš„ä¼˜åŠ¿ã€‚å…³é”®è¯æœç´¢æ“…é•¿ç²¾ç¡®åŒ¹é…ï¼Œå‘é‡æœç´¢æ“…é•¿è¯­ä¹‰ç†è§£ã€‚",
]

# ç”ŸæˆåµŒå…¥å¹¶ç´¢å¼•
print("ç”ŸæˆåµŒå…¥å‘é‡å¹¶ç´¢å¼•æ–‡æ¡£...")
for i, doc in enumerate(documents):
    embedding = model.encode(doc).tolist()
    es.index(
        index=INDEX_NAME,
        id=str(i),
        body={
            "text": doc,
            "embedding": embedding
        }
    )
es.indices.refresh(index=INDEX_NAME)
print(f"âœ“ å·²ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£")

# æŸ¥è¯¢å‡½æ•°
def rag_query(question: str) -> str:
    """RAG æŸ¥è¯¢æµç¨‹"""
    # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = model.encode(question).tolist()
    
    # 2. å‘é‡æœç´¢
    search_result = es.search(
        index=INDEX_NAME,
        body={
            "knn": {
                "field": "embedding",
                "query_vector": query_embedding,
                "k": 3,
                "num_candidates": 10
            },
            "_source": ["text"]
        }
    )
    
    # 3. æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£
    retrieved_docs = []
    for hit in search_result['hits']['hits']:
        retrieved_docs.append({
            'text': hit['_source']['text'],
            'score': hit['_score']
        })
    
    if not retrieved_docs:
        return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
    
    # 4. æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n".join([f"[æ–‡æ¡£{i+1}] {doc['text']}" for i, doc in enumerate(retrieved_docs)])
    
    # 5. ä½¿ç”¨ GPT ç”Ÿæˆç­”æ¡ˆ
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªhelpfulçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨ç­”æ¡ˆä¸­æ ‡æ³¨å¼•ç”¨æ¥æºï¼ˆå¦‚[æ–‡æ¡£1]ï¼‰ã€‚"
            },
            {
                "role": "user",
                "content": f"é—®é¢˜: {question}\n\nå‚è€ƒæ–‡æ¡£:\n{context}\n\nè¯·å›ç­”:"
            }
        ],
        temperature=0.7,
        max_tokens=500
    )
    
    answer = response.choices[0].message.content
    return answer, retrieved_docs

# æ¼”ç¤ºæŸ¥è¯¢
print("\n[4/6] ç³»ç»Ÿå°±ç»ªï¼")
print("="*70)
print("ç°åœ¨å¯ä»¥æé—®äº†ï¼\n")

# ç¤ºä¾‹é—®é¢˜
example_questions = [
    "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    "Elasticsearch çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "ä»€ä¹ˆæ˜¯å‘é‡åµŒå…¥ï¼Ÿ"
]

print("ç¤ºä¾‹é—®é¢˜:")
for i, q in enumerate(example_questions, 1):
    print(f"  {i}. {q}")

print("\n" + "="*70)
print("è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰:")
print("="*70 + "\n")

# äº¤äº’å¼é—®ç­”
while True:
    try:
        question = input("ğŸ’¬ ä½ çš„é—®é¢˜: ").strip()
        
        if not question:
            continue
        
        if question.lower() in ['exit', 'quit', 'q']:
            print("\nå†è§ï¼")
            break
        
        print("\nğŸ” æ­£åœ¨æ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆ...\n")
        
        answer, docs = rag_query(question)
        
        print("="*70)
        print("ğŸ“ AI å›ç­”:")
        print("="*70)
        print(answer)
        
        print("\n" + "="*70)
        print(f"ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£ (å…±{len(docs)}ä¸ª):")
        print("="*70)
        for i, doc in enumerate(docs, 1):
            print(f"\n[æ–‡æ¡£{i}] (ç›¸ä¼¼åº¦: {doc['score']:.4f})")
            print(doc['text'][:150] + "...")
        
        print("\n" + "="*70 + "\n")
        
    except KeyboardInterrupt:
        print("\n\nå†è§ï¼")
        break
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}\n")

# æ¸…ç†
print("\n[5/6] æ¸…ç†...")
cleanup = input("æ˜¯å¦åˆ é™¤æ¼”ç¤ºç´¢å¼•ï¼Ÿ(y/n): ").strip().lower()
if cleanup == 'y':
    es.indices.delete(index=INDEX_NAME)
    print(f"âœ“ å·²åˆ é™¤ç´¢å¼•: {INDEX_NAME}")

print("\n[6/6] å®Œæˆï¼")
print("="*70)
print("ğŸ‰ RAG ç³»ç»Ÿæ¼”ç¤ºç»“æŸï¼")
print("="*70)

